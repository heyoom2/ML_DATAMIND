{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6_vaTLOZHcQ",
        "outputId": "2d755f73-7c88-4f90-cddb-84020c8fda82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded all feature files successfully!\n",
            "\n",
            "Monitored shapes: (19000, 11) (19000, 10) (19000, 4)\n",
            "Unmonitored shapes: (3000, 11) (3000, 10) (3000, 4)\n",
            "\n",
            "Combined Monitored shape: (19000, 25)\n",
            "Combined Unmonitored shape: (3000, 25)\n",
            "\n",
            "Monitored labels shape: (19000,)\n",
            "Unmonitored labels shape: (3000,)\n",
            "\n",
            "Total dataset shape: (22000, 25)\n",
            "Total label shape: (22000,)\n",
            "\n",
            "Train shape: (16500, 25), Test shape: (5500, 25)\n",
            "\n",
            "Feature scaling completed!\n",
            "Saved all processed data! Ready for model training.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ê²½ë¡œ ì„¤ì •\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Google Drive ê²½ë¡œ (í•„ìš”ì— ë§ê²Œ ìˆ˜ì •)\n",
        "BASE_PATH = \"/content/drive/MyDrive/ML_Dataset\"\n",
        "\n",
        "# 2. Feature íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "X_burst_mon = np.load(f\"{BASE_PATH}/features_burst_mon.npy\")\n",
        "X_cum_mon   = np.load(f\"{BASE_PATH}/features_cum_mon.npy\")\n",
        "X_inout_mon = np.load(f\"{BASE_PATH}/features_inout_mon.npy\")\n",
        "\n",
        "X_burst_unmon = np.load(f\"{BASE_PATH}/features_burst_unmon.npy\")\n",
        "X_cum_unmon   = np.load(f\"{BASE_PATH}/features_cum_unmon.npy\")\n",
        "X_inout_unmon = np.load(f\"{BASE_PATH}/features_inout_unmon.npy\")\n",
        "\n",
        "print(\"Loaded all feature files successfully!\\n\")\n",
        "print(\"Monitored shapes:\", X_burst_mon.shape, X_cum_mon.shape, X_inout_mon.shape)\n",
        "print(\"Unmonitored shapes:\", X_burst_unmon.shape, X_cum_unmon.shape, X_inout_unmon.shape)\n",
        "\n",
        "# 3. Feature ë³‘í•© (ì—´ ê¸°ì¤€)\n",
        "X_mon = np.concatenate([X_burst_mon, X_cum_mon, X_inout_mon], axis=1)\n",
        "X_unmon = np.concatenate([X_burst_unmon, X_cum_unmon, X_inout_unmon], axis=1)\n",
        "\n",
        "print(f\"\\nCombined Monitored shape: {X_mon.shape}\")\n",
        "print(f\"Combined Unmonitored shape: {X_unmon.shape}\")\n",
        "\n",
        "# 4. Label ìƒì„±\n",
        "# Monitored: 95ê°œ í´ë˜ìŠ¤ (0~94)\n",
        "# Unmonitored: -1 label\n",
        "y_mon = np.repeat(np.arange(95), 200)\n",
        "y_unmon = np.full(X_unmon.shape[0], -1)\n",
        "\n",
        "print(f\"\\nMonitored labels shape: {y_mon.shape}\")\n",
        "print(f\"Unmonitored labels shape: {y_unmon.shape}\")\n",
        "\n",
        "# 5. Open-Worldìš© ì „ì²´ ë°ì´í„° ê²°í•©\n",
        "X_total = np.concatenate([X_mon, X_unmon], axis=0)\n",
        "y_total = np.concatenate([y_mon, y_unmon], axis=0)\n",
        "\n",
        "print(\"\\nTotal dataset shape:\", X_total.shape)\n",
        "print(\"Total label shape:\", y_total.shape)\n",
        "\n",
        "# 6. Train/Test Data Split\n",
        "# Stratified samplingì€ ë‹¤ì¤‘í´ë˜ìŠ¤ì—ë§Œ ê°€ëŠ¥í•˜ë¯€ë¡œ, open-world binary ì„¤ì •ìœ¼ë¡œ ë¶„í• \n",
        "y_binary = np.where(y_total == -1, 0, 1)  # monitored=1, unmonitored=0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_total, y_binary, test_size=0.25, random_state=42, stratify=y_binary\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "# 7. Feature Scaling (í‘œì¤€í™”)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nFeature scaling completed!\")\n",
        "\n",
        "\n",
        "# 8. Save\n",
        "np.save(f\"{BASE_PATH}/X_train_scaled.npy\", X_train_scaled)\n",
        "np.save(f\"{BASE_PATH}/X_test_scaled.npy\", X_test_scaled)\n",
        "np.save(f\"{BASE_PATH}/y_train.npy\", y_train)\n",
        "np.save(f\"{BASE_PATH}/y_test.npy\", y_test)\n",
        "\n",
        "print(\"Saved all processed data! Ready for model training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    \"X_train_scaled\": f\"{BASE_PATH}/X_train_scaled.npy\",\n",
        "    \"X_test_scaled\": f\"{BASE_PATH}/X_test_scaled.npy\",\n",
        "    \"y_train\": f\"{BASE_PATH}/y_train.npy\",\n",
        "    \"y_test\": f\"{BASE_PATH}/y_test.npy\",\n",
        "}\n",
        "\n",
        "for name, path in files.items():\n",
        "    arr = np.load(path)\n",
        "    print(f\"\\nğŸ“ {name}\")\n",
        "    print(f\"Shape: {arr.shape}\")\n",
        "    print(f\"Dtype: {arr.dtype}\")\n",
        "\n",
        "    # ìˆ«ìí˜• ë°°ì—´ë§Œ í†µê³„ ì¶œë ¥\n",
        "    if np.issubdtype(arr.dtype, np.number):\n",
        "        print(f\"Mean: {np.mean(arr):.4f}, Std: {np.std(arr):.4f}\")\n",
        "        print(f\"Min: {np.min(arr):.4f}, Max: {np.max(arr):.4f}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "y_train = np.load(files[\"y_train\"])\n",
        "y_test = np.load(files[\"y_test\"])\n",
        "\n",
        "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "\n",
        "print(\"\\n Label Distribution (Train):\")\n",
        "for u, c in zip(unique_train, counts_train):\n",
        "    print(f\"  Class {int(u)}: {c} samples\")\n",
        "\n",
        "print(\"\\n Label Distribution (Test):\")\n",
        "for u, c in zip(unique_test, counts_test):\n",
        "    print(f\"  Class {int(u)}: {c} samples\")\n",
        "\n",
        "print(\"\\n Dataset integrity check complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1AWYai6cX9-",
        "outputId": "4ba2bfa2-7203-47df-d9be-64f9826ce07c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“ X_train_scaled\n",
            "Shape: (16500, 25)\n",
            "Dtype: float64\n",
            "Mean: 0.0000, Std: 0.9592\n",
            "Min: -16.3972, Max: 128.4484\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ X_test_scaled\n",
            "Shape: (5500, 25)\n",
            "Dtype: float64\n",
            "Mean: 0.0010, Std: 1.0114\n",
            "Min: -16.3972, Max: 173.9198\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ y_train\n",
            "Shape: (16500,)\n",
            "Dtype: int64\n",
            "Mean: 0.8636, Std: 0.3432\n",
            "Min: 0.0000, Max: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "ğŸ“ y_test\n",
            "Shape: (5500,)\n",
            "Dtype: int64\n",
            "Mean: 0.8636, Std: 0.3432\n",
            "Min: 0.0000, Max: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            " Label Distribution (Train):\n",
            "  Class 0: 2250 samples\n",
            "  Class 1: 14250 samples\n",
            "\n",
            " Label Distribution (Test):\n",
            "  Class 0: 750 samples\n",
            "  Class 1: 4750 samples\n",
            "\n",
            " Dataset integrity check complete!\n"
          ]
        }
      ]
    }
  ]
}